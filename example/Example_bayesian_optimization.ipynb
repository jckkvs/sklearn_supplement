{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd \\\\sodfsv10\\先端解析Ｇ\\01_計算科学DB\\080.チーム共有\\90.AI開発T\\解析プログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn_expansion.experimental_design import create_optimal_design\n",
    "from sklearn_expansion.experimental_design import BayesianOptimization\n",
    "from sklearn_expansion import experimental_design\n",
    "from sklearn_expansion.ensemble import MultiRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(432, 10)\n",
      "(10,)\n",
      "(432,)\n",
      "346.0\n",
      "25.0\n",
      "[ 0.03807591  0.05068012  0.06169621  0.02187235 -0.0442235  -0.03482076\n",
      " -0.04340085 -0.00259226  0.01990842 -0.01764613]\n",
      "216.0\n",
      "61.0\n"
     ]
    }
   ],
   "source": [
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=10)\n",
    "X_design_space = X_test\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "n_y = 1\n",
    "print(max(y))\n",
    "print(min(y))\n",
    "print(X[0])\n",
    "\n",
    "print(max(y_train))\n",
    "print(min(y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 完全にデータを学習した決定木を作成しておく\n",
    "# ベイズ最適化結果の答え合わせ用\n",
    "tree = DecisionTreeRegressor(max_depth=30)\n",
    "tree.fit(X,y)\n",
    "print(mean_absolute_error(y, tree.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>-0.049105</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.160855</td>\n",
       "      <td>-0.046985</td>\n",
       "      <td>-0.029088</td>\n",
       "      <td>-0.019790</td>\n",
       "      <td>-0.047082</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.028017</td>\n",
       "      <td>0.011349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.034443</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.125287</td>\n",
       "      <td>0.028758</td>\n",
       "      <td>-0.053855</td>\n",
       "      <td>-0.012900</td>\n",
       "      <td>-0.102307</td>\n",
       "      <td>0.108111</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.027917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.034443</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.111276</td>\n",
       "      <td>0.076958</td>\n",
       "      <td>-0.031840</td>\n",
       "      <td>-0.033881</td>\n",
       "      <td>-0.021311</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.028017</td>\n",
       "      <td>0.073480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.059871</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.076786</td>\n",
       "      <td>0.025315</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.016849</td>\n",
       "      <td>-0.054446</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.029936</td>\n",
       "      <td>0.044485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.019913</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.104809</td>\n",
       "      <td>0.070073</td>\n",
       "      <td>-0.035968</td>\n",
       "      <td>-0.026679</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.040343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>-0.027310</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.047685</td>\n",
       "      <td>-0.046985</td>\n",
       "      <td>0.034206</td>\n",
       "      <td>0.057245</td>\n",
       "      <td>-0.080217</td>\n",
       "      <td>0.130252</td>\n",
       "      <td>0.045066</td>\n",
       "      <td>0.131470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.032201</td>\n",
       "      <td>0.006687</td>\n",
       "      <td>0.017475</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.061054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.070900</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>-0.033214</td>\n",
       "      <td>-0.012577</td>\n",
       "      <td>-0.034508</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.067736</td>\n",
       "      <td>-0.013504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.048974</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.088642</td>\n",
       "      <td>0.087287</td>\n",
       "      <td>0.035582</td>\n",
       "      <td>0.021546</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.066048</td>\n",
       "      <td>0.131470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.030811</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.056307</td>\n",
       "      <td>0.076958</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>-0.012274</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.120053</td>\n",
       "      <td>0.090049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>-0.016412</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.127443</td>\n",
       "      <td>0.097616</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.017475</td>\n",
       "      <td>-0.021311</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.034864</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>-0.020045</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.085408</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.091996</td>\n",
       "      <td>0.089499</td>\n",
       "      <td>-0.061809</td>\n",
       "      <td>0.145012</td>\n",
       "      <td>0.080948</td>\n",
       "      <td>0.052770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.067136</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.056307</td>\n",
       "      <td>0.073515</td>\n",
       "      <td>-0.013953</td>\n",
       "      <td>-0.039205</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.075738</td>\n",
       "      <td>0.036201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.092695</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.028284</td>\n",
       "      <td>-0.015999</td>\n",
       "      <td>0.036958</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>0.056003</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.005145</td>\n",
       "      <td>-0.001078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.019913</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.014272</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>0.014942</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>-0.047082</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.046661</td>\n",
       "      <td>0.090049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "256 -0.049105 -0.044642  0.160855 -0.046985 -0.029088 -0.019790 -0.047082   \n",
       "32   0.034443  0.050680  0.125287  0.028758 -0.053855 -0.012900 -0.102307   \n",
       "138  0.034443  0.050680  0.111276  0.076958 -0.031840 -0.033881 -0.021311   \n",
       "290  0.059871  0.050680  0.076786  0.025315  0.001183  0.016849 -0.054446   \n",
       "362  0.019913  0.050680  0.104809  0.070073 -0.035968 -0.026679 -0.024993   \n",
       "141 -0.027310 -0.044642  0.047685 -0.046985  0.034206  0.057245 -0.080217   \n",
       "359  0.038076  0.050680  0.005650  0.032201  0.006687  0.017475 -0.024993   \n",
       "9   -0.070900 -0.044642  0.039062 -0.033214 -0.012577 -0.034508 -0.024993   \n",
       "428  0.048974  0.050680  0.088642  0.087287  0.035582  0.021546 -0.024993   \n",
       "254  0.030811  0.050680  0.056307  0.076958  0.049341 -0.012274 -0.036038   \n",
       "262 -0.016412  0.050680  0.127443  0.097616  0.016318  0.017475 -0.021311   \n",
       "336 -0.020045 -0.044642  0.085408 -0.036656  0.091996  0.089499 -0.061809   \n",
       "250  0.067136 -0.044642  0.056307  0.073515 -0.013953 -0.039205 -0.032356   \n",
       "102 -0.092695 -0.044642  0.028284 -0.015999  0.036958  0.024991  0.056003   \n",
       "113  0.019913  0.050680  0.014272  0.063187  0.014942  0.020293 -0.047082   \n",
       "\n",
       "            7         8         9  \n",
       "256  0.034309  0.028017  0.011349  \n",
       "32   0.108111  0.000271  0.027917  \n",
       "138 -0.002592  0.028017  0.073480  \n",
       "290  0.034309  0.029936  0.044485  \n",
       "362 -0.002592  0.003712  0.040343  \n",
       "141  0.130252  0.045066  0.131470  \n",
       "359  0.034309  0.014823  0.061054  \n",
       "9   -0.002592  0.067736 -0.013504  \n",
       "428  0.034309  0.066048  0.131470  \n",
       "254  0.071210  0.120053  0.090049  \n",
       "262  0.034309  0.034864  0.003064  \n",
       "336  0.145012  0.080948  0.052770  \n",
       "250 -0.002592  0.075738  0.036201  \n",
       "102 -0.039493 -0.005145 -0.001078  \n",
       "113  0.034309  0.046661  0.090049  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yが高い30データを抽出\n",
    "Xy_df = pd.DataFrame(np.concatenate([X,y.reshape(-1,1)], axis=1))\n",
    "Xy_columns = Xy_df.columns\n",
    "Xy_df = Xy_df.sort_values(by=Xy_columns[-1], ascending=False)\n",
    "\n",
    "high_y_df = Xy_df.iloc[:15,:]\n",
    "X_high = high_y_df.iloc[:,:-1]\n",
    "y_high = high_y_df.iloc[:,-1]\n",
    "X_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1**2 * RBF(length_scale=1) + WhiteKernel(noise_level=1) + 1**2 * DotProduct(sigma_0=1)\n"
     ]
    }
   ],
   "source": [
    "kernels = BayesianOptimization.kernels()\n",
    "kernel = kernels[4]\n",
    "bo = BayesianOptimization()\n",
    "print(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__k2__sigma_0 is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__k2__sigma_0 is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__k2__sigma_0 is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__k2__sigma_0 is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__k2__sigma_0 is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "n_y=1\n",
    "n_suggestion = 15\n",
    "estimator = GaussianProcessRegressor(alpha=0,\n",
    "                                             kernel=kernel,\n",
    "                                             random_state=None,\n",
    "                                             optimizer='fmin_l_bfgs_b')\n",
    "\n",
    "\n",
    "\n",
    "target_ranges = [{\"lower\":250, \"upper\":np.inf}]\n",
    "\n",
    "next_Xs = bo.kriging_believer_algorithm(estimator=estimator,\n",
    "                                        X=X_train,\n",
    "                                        y=y_train,\n",
    "                                        n_y=n_y,\n",
    "                                        X_design=X_design_space,\n",
    "                                        target_ranges=target_ranges,\n",
    "                                        acquisition_function=\"PTR\",\n",
    "                                        n_suggestion=n_suggestion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.096197</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.051996</td>\n",
       "      <td>0.079254</td>\n",
       "      <td>0.054845</td>\n",
       "      <td>0.036577</td>\n",
       "      <td>-0.076536</td>\n",
       "      <td>0.141322</td>\n",
       "      <td>0.098646</td>\n",
       "      <td>0.061054</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.074401</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.031517</td>\n",
       "      <td>0.101058</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.036890</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.033657</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.074401</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.034751</td>\n",
       "      <td>0.094173</td>\n",
       "      <td>0.057597</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.073802</td>\n",
       "      <td>-0.021788</td>\n",
       "      <td>236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074401</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.114509</td>\n",
       "      <td>0.028758</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>-0.005220</td>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.074401</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>0.061725</td>\n",
       "      <td>0.042840</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.058039</td>\n",
       "      <td>-0.059067</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.034751</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>0.152538</td>\n",
       "      <td>0.198788</td>\n",
       "      <td>-0.061809</td>\n",
       "      <td>0.185234</td>\n",
       "      <td>0.015567</td>\n",
       "      <td>0.073480</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.063504</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.088642</td>\n",
       "      <td>0.070073</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>0.037517</td>\n",
       "      <td>-0.050764</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.073480</td>\n",
       "      <td>264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.074401</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.085408</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>0.014942</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.006209</td>\n",
       "      <td>0.085907</td>\n",
       "      <td>261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.063504</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>0.066630</td>\n",
       "      <td>0.090620</td>\n",
       "      <td>0.108914</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>0.017703</td>\n",
       "      <td>-0.035817</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.059871</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.021295</td>\n",
       "      <td>0.087287</td>\n",
       "      <td>0.045213</td>\n",
       "      <td>0.031567</td>\n",
       "      <td>-0.047082</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.079121</td>\n",
       "      <td>0.135612</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.030811</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.104809</td>\n",
       "      <td>0.076958</td>\n",
       "      <td>-0.011201</td>\n",
       "      <td>-0.011335</td>\n",
       "      <td>-0.058127</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.057104</td>\n",
       "      <td>0.036201</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.071397</td>\n",
       "      <td>0.097616</td>\n",
       "      <td>0.087868</td>\n",
       "      <td>0.075407</td>\n",
       "      <td>-0.021311</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.071424</td>\n",
       "      <td>0.023775</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.033673</td>\n",
       "      <td>0.125158</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>0.026243</td>\n",
       "      <td>-0.010266</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.026714</td>\n",
       "      <td>0.061054</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.041840</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.047685</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>0.127771</td>\n",
       "      <td>0.128016</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>0.108111</td>\n",
       "      <td>0.063893</td>\n",
       "      <td>0.040343</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.063504</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.050396</td>\n",
       "      <td>0.107944</td>\n",
       "      <td>0.031454</td>\n",
       "      <td>0.019354</td>\n",
       "      <td>-0.017629</td>\n",
       "      <td>0.023608</td>\n",
       "      <td>0.058039</td>\n",
       "      <td>0.040343</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.096197 -0.044642  0.051996  0.079254  0.054845  0.036577 -0.076536   \n",
       "1   0.074401 -0.044642  0.031517  0.101058  0.046589  0.036890  0.015505   \n",
       "2   0.074401 -0.044642  0.034751  0.094173  0.057597  0.020293  0.022869   \n",
       "3   0.074401 -0.044642  0.114509  0.028758  0.024574  0.024991  0.019187   \n",
       "4   0.074401 -0.044642  0.018584  0.063187  0.061725  0.042840  0.008142   \n",
       "5   0.005383  0.050680  0.034751 -0.001080  0.152538  0.198788 -0.061809   \n",
       "6   0.063504  0.050680  0.088642  0.070073  0.020446  0.037517 -0.050764   \n",
       "7   0.074401 -0.044642  0.085408  0.063187  0.014942  0.013091  0.015505   \n",
       "8   0.063504  0.050680 -0.001895  0.066630  0.090620  0.108914  0.022869   \n",
       "9   0.059871 -0.044642 -0.021295  0.087287  0.045213  0.031567 -0.047082   \n",
       "10  0.030811 -0.044642  0.104809  0.076958 -0.011201 -0.011335 -0.058127   \n",
       "11 -0.001882  0.050680  0.071397  0.097616  0.087868  0.075407 -0.021311   \n",
       "12 -0.001882 -0.044642  0.033673  0.125158  0.024574  0.026243 -0.010266   \n",
       "13 -0.041840 -0.044642  0.047685  0.059744  0.127771  0.128016 -0.024993   \n",
       "14  0.063504 -0.044642 -0.050396  0.107944  0.031454  0.019354 -0.017629   \n",
       "\n",
       "           7         8         9      0  \n",
       "0   0.141322  0.098646  0.061054  230.0  \n",
       "1  -0.002592  0.033657  0.044485  296.0  \n",
       "2  -0.002592  0.073802 -0.021788  236.0  \n",
       "3  -0.002592 -0.000609 -0.005220  237.0  \n",
       "4  -0.002592  0.058039 -0.059067  248.0  \n",
       "5   0.185234  0.015567  0.073480   84.0  \n",
       "6   0.071210  0.029300  0.073480  264.0  \n",
       "7  -0.002592  0.006209  0.085907  261.0  \n",
       "8   0.017703 -0.035817  0.003064   63.0  \n",
       "9   0.071210  0.079121  0.135612  281.0  \n",
       "10  0.034309  0.057104  0.036201  270.0  \n",
       "11  0.071210  0.071424  0.023775  252.0  \n",
       "12 -0.002592  0.026714  0.061054  270.0  \n",
       "13  0.108111  0.063893  0.040343  258.0  \n",
       "14  0.023608  0.058039  0.040343  189.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PTR_candidates = pd.DataFrame(next_Xs)\n",
    "PTR_candidates\n",
    "PTR_predicts = pd.DataFrame(tree.predict(PTR_candidates))\n",
    "\n",
    "pd.concat([PTR_candidates, PTR_predicts], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "n_y=1\n",
    "n_suggestion = 15\n",
    "estimator = SVR()\n",
    "Bag_estimator = BaggingRegressor(estimator, n_estimators=10)\n",
    "Multi_estimator = MultiRegressor(Bag_estimator)\n",
    "target_ranges = [{\"lower\":250, \"upper\":np.inf}]\n",
    "\n",
    "next_Xs = bo.kriging_believer_algorithm(estimator=Multi_estimator,\n",
    "                                        X=X_train,\n",
    "                                        y=y_train,\n",
    "                                        n_y=n_y,\n",
    "                                        X_design=X_design_space,\n",
    "                                        target_ranges=target_ranges,\n",
    "                                        acquisition_function=\"PTR\",\n",
    "                                        n_suggestion=n_suggestion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.078165</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.040696</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>-0.100638</td>\n",
       "      <td>-0.112795</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>-0.020289</td>\n",
       "      <td>-0.050783</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.020045</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>0.090730</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>0.037595</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.057800</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.023677</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.030440</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>0.082364</td>\n",
       "      <td>0.092004</td>\n",
       "      <td>-0.017629</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.033047</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.061174</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.055231</td>\n",
       "      <td>-0.054549</td>\n",
       "      <td>0.041277</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>-0.093936</td>\n",
       "      <td>-0.054925</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.023546</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.110198</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>-0.032942</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>0.020655</td>\n",
       "      <td>0.099240</td>\n",
       "      <td>0.023775</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.092695</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.081653</td>\n",
       "      <td>-0.057314</td>\n",
       "      <td>-0.060735</td>\n",
       "      <td>-0.068014</td>\n",
       "      <td>0.048640</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>-0.066488</td>\n",
       "      <td>-0.021788</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.022373</td>\n",
       "      <td>0.028758</td>\n",
       "      <td>-0.066239</td>\n",
       "      <td>-0.045155</td>\n",
       "      <td>-0.061809</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.054925</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.020045</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>0.097616</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>-0.020729</td>\n",
       "      <td>0.063367</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>0.012553</td>\n",
       "      <td>0.011349</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.034575</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.037463</td>\n",
       "      <td>-0.060757</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>0.043466</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.030751</td>\n",
       "      <td>-0.071494</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.012648</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.033673</td>\n",
       "      <td>0.033349</td>\n",
       "      <td>0.030078</td>\n",
       "      <td>0.027183</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.027917</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.060003</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.047163</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>-0.071743</td>\n",
       "      <td>-0.057681</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.062913</td>\n",
       "      <td>-0.054925</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.074401</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.114509</td>\n",
       "      <td>0.028758</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>-0.005220</td>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.049840</td>\n",
       "      <td>0.097616</td>\n",
       "      <td>-0.015328</td>\n",
       "      <td>-0.016345</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.017037</td>\n",
       "      <td>-0.013504</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0  -0.078165 -0.044642 -0.040696 -0.081414 -0.100638 -0.112795  0.022869   \n",
       "1  -0.020045 -0.044642  0.018584  0.090730  0.003935  0.008707  0.037595   \n",
       "2  -0.023677 -0.044642  0.030440 -0.005671  0.082364  0.092004 -0.017629   \n",
       "3  -0.089063 -0.044642 -0.061174 -0.026328 -0.055231 -0.054549  0.041277   \n",
       "4   0.023546 -0.044642  0.110198  0.063187  0.013567 -0.032942 -0.024993   \n",
       "5   0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "6  -0.092695 -0.044642 -0.081653 -0.057314 -0.060735 -0.068014  0.048640   \n",
       "7   0.041708  0.050680 -0.022373  0.028758 -0.066239 -0.045155 -0.061809   \n",
       "8  -0.020045 -0.044642  0.004572  0.097616  0.005311 -0.020729  0.063367   \n",
       "9  -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "10 -0.034575 -0.044642 -0.037463 -0.060757  0.020446  0.043466 -0.013948   \n",
       "11  0.012648 -0.044642  0.033673  0.033349  0.030078  0.027183 -0.002903   \n",
       "12 -0.060003  0.050680 -0.047163 -0.022885 -0.071743 -0.057681 -0.006584   \n",
       "13  0.074401 -0.044642  0.114509  0.028758  0.024574  0.024991  0.019187   \n",
       "14  0.005383 -0.044642  0.049840  0.097616 -0.015328 -0.016345 -0.006584   \n",
       "\n",
       "           7         8         9      0  \n",
       "0  -0.076395 -0.020289 -0.050783  152.0  \n",
       "1  -0.039493 -0.057800  0.007207  113.0  \n",
       "2   0.071210  0.033047  0.003064  122.0  \n",
       "3  -0.076395 -0.093936 -0.054925   99.0  \n",
       "4   0.020655  0.099240  0.023775  258.0  \n",
       "5  -0.002592 -0.031991 -0.046641  135.0  \n",
       "6  -0.076395 -0.066488 -0.021788   85.0  \n",
       "7  -0.002592  0.002864 -0.054925  156.0  \n",
       "8  -0.039493  0.012553  0.011349   48.0  \n",
       "9   0.026560  0.044528 -0.025930  220.0  \n",
       "10 -0.002592 -0.030751 -0.071494  128.0  \n",
       "11  0.008847  0.031193  0.027917  198.0  \n",
       "12 -0.039493 -0.062913 -0.054925   72.0  \n",
       "13 -0.002592 -0.000609 -0.005220  237.0  \n",
       "14 -0.002592  0.017037 -0.013504  280.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVR_candidates = pd.DataFrame(next_Xs)\n",
    "SVR_candidates\n",
    "SVR_predicts = pd.DataFrame(tree.predict(SVR_candidates))\n",
    "\n",
    "pd.concat([SVR_candidates, SVR_predicts], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__k2__sigma_0 is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__k2__sigma_0 is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__k2__sigma_0 is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__k1__constant_value is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\11665307\\Anaconda3\\envs\\py37_test\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:409: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__k2__sigma_0 is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "n_y=1\n",
    "n_suggestion = 15\n",
    "estimator = model = GaussianProcessRegressor(alpha=0,\n",
    "                                             kernel=kernel,\n",
    "                                             random_state=None,\n",
    "                                             optimizer='fmin_l_bfgs_b')\n",
    "\n",
    "\n",
    "\n",
    "next_Xs = bo.kriging_believer_algorithm(estimator=estimator,\n",
    "                                        X=X_train,\n",
    "                                        y=y_train,\n",
    "                                        n_y=n_y,\n",
    "                                        X_design=X_design_space,\n",
    "                                        acquisition_function=\"EI\",\n",
    "                                        n_suggestion=n_suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067136</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.006206</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>-0.042848</td>\n",
       "      <td>-0.095885</td>\n",
       "      <td>0.052322</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>0.059424</td>\n",
       "      <td>0.052770</td>\n",
       "      <td>283.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.096197</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.051996</td>\n",
       "      <td>0.079254</td>\n",
       "      <td>0.054845</td>\n",
       "      <td>0.036577</td>\n",
       "      <td>-0.076536</td>\n",
       "      <td>0.141322</td>\n",
       "      <td>0.098646</td>\n",
       "      <td>0.061054</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.081666</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.035644</td>\n",
       "      <td>0.126395</td>\n",
       "      <td>0.091065</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.084495</td>\n",
       "      <td>-0.030072</td>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023546</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.062039</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>-0.036073</td>\n",
       "      <td>-0.091262</td>\n",
       "      <td>0.155345</td>\n",
       "      <td>0.133396</td>\n",
       "      <td>0.081764</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048974</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.123131</td>\n",
       "      <td>0.083844</td>\n",
       "      <td>-0.104765</td>\n",
       "      <td>-0.100895</td>\n",
       "      <td>-0.069172</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.036646</td>\n",
       "      <td>-0.030072</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.034751</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>0.152538</td>\n",
       "      <td>0.198788</td>\n",
       "      <td>-0.061809</td>\n",
       "      <td>0.185234</td>\n",
       "      <td>0.015567</td>\n",
       "      <td>0.073480</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.064408</td>\n",
       "      <td>0.035644</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>-0.057994</td>\n",
       "      <td>0.181179</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>-0.050783</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.060003</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.047163</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>-0.071743</td>\n",
       "      <td>-0.057681</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.062913</td>\n",
       "      <td>-0.054925</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.074401</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.114509</td>\n",
       "      <td>0.028758</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>-0.005220</td>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>0.039087</td>\n",
       "      <td>0.017694</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.016305</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>265.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.045032</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.049769</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.045341</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.019140</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.027326</td>\n",
       "      <td>-0.013527</td>\n",
       "      <td>0.100183</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>0.017763</td>\n",
       "      <td>-0.013504</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.110727</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>0.028758</td>\n",
       "      <td>-0.027712</td>\n",
       "      <td>-0.007264</td>\n",
       "      <td>-0.047082</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.077622</td>\n",
       "      <td>277.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.023973</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>-0.034592</td>\n",
       "      <td>-0.038892</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.015998</td>\n",
       "      <td>-0.013504</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.067136</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.036907</td>\n",
       "      <td>-0.050428</td>\n",
       "      <td>-0.023584</td>\n",
       "      <td>-0.034508</td>\n",
       "      <td>0.048640</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.025952</td>\n",
       "      <td>-0.038357</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.067136  0.050680 -0.006206  0.063187 -0.042848 -0.095885  0.052322   \n",
       "1   0.096197 -0.044642  0.051996  0.079254  0.054845  0.036577 -0.076536   \n",
       "2   0.081666  0.050680  0.001339  0.035644  0.126395  0.091065  0.019187   \n",
       "3   0.023546  0.050680  0.061696  0.062039  0.024574 -0.036073 -0.091262   \n",
       "4   0.048974  0.050680  0.123131  0.083844 -0.104765 -0.100895 -0.069172   \n",
       "5   0.005383  0.050680  0.034751 -0.001080  0.152538  0.198788 -0.061809   \n",
       "6   0.041708 -0.044642 -0.064408  0.035644  0.012191 -0.057994  0.181179   \n",
       "7  -0.060003  0.050680 -0.047163 -0.022885 -0.071743 -0.057681 -0.006584   \n",
       "8   0.074401 -0.044642  0.114509  0.028758  0.024574  0.024991  0.019187   \n",
       "9   0.009016  0.050680  0.018584  0.039087  0.017694  0.010586  0.019187   \n",
       "10  0.038076  0.050680  0.016428  0.021872  0.039710  0.045032 -0.043401   \n",
       "11  0.045341 -0.044642 -0.019140  0.021872  0.027326 -0.013527  0.100183   \n",
       "12  0.110727  0.050680  0.006728  0.028758 -0.027712 -0.007264 -0.047082   \n",
       "13 -0.005515 -0.044642  0.023973  0.008101 -0.034592 -0.038892  0.022869   \n",
       "14  0.067136 -0.044642  0.036907 -0.050428 -0.023584 -0.034508  0.048640   \n",
       "\n",
       "           7         8         9      0  \n",
       "0  -0.076395  0.059424  0.052770  283.0  \n",
       "1   0.141322  0.098646  0.061054  230.0  \n",
       "2   0.034309  0.084495 -0.030072  196.0  \n",
       "3   0.155345  0.133396  0.081764  242.0  \n",
       "4  -0.002592  0.036646 -0.030072  281.0  \n",
       "5   0.185234  0.015567  0.073480   84.0  \n",
       "6  -0.076395 -0.000609 -0.050783  170.0  \n",
       "7  -0.039493 -0.062913 -0.054925   72.0  \n",
       "8  -0.002592 -0.000609 -0.005220  237.0  \n",
       "9  -0.002592  0.016305 -0.017646  265.0  \n",
       "10  0.071210  0.049769  0.015491  212.0  \n",
       "11 -0.039493  0.017763 -0.013504  214.0  \n",
       "12  0.034309  0.002008  0.077622  277.0  \n",
       "13 -0.039493 -0.015998 -0.013504  121.0  \n",
       "14 -0.039493 -0.025952 -0.038357   90.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EI_candidates = pd.DataFrame(next_Xs)\n",
    "EI_candidates\n",
    "EI_predicts = pd.DataFrame(tree.predict(EI_candidates))\n",
    "\n",
    "pd.concat([EI_candidates, EI_predicts], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca =PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_ptr_pca = pca.transform(PTR_candidates)\n",
    "X_ei_pca = pca.transform(EI_candidates)\n",
    "X_high_pca = pca.transform(X_high)\n",
    "X_SVR_PCA = pca.transform(SVR_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2c49be66308>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAboAAAGbCAYAAACyB1UWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAoX0lEQVR4nO3df3Bdd3nn8ffjGIeVnZKE2CZKnN64pGMbWkQqwma7sLIdx7ELhJlNIV0g0gITd7d0dqfbHcKwrG3oTmE7Hdg/2CEGikRpgUAH8NDG2WBbDM0GGgWpQJzQOEatHYXITXBJajYm8Xf/OEdEFpIt6f4495z7fs3cufeec67u42tJH33Pj+cbKSUkSaqqJUUXIElSMxl0kqRKM+gkSZVm0EmSKs2gkyRV2tKiC1iMSy65JNVqtaLLkCS1ifvvv/8fU0orZ1tXyqCr1WqMjIwUXYYkqU1ExN/Ptc5dl5KkSjPoJEmVZtBJkirNoJMkVZpBJ0mqNINOklRpBp0kqdIMOklSpTUk6CLihoj4fkQcjojbZln/2oj4dkQ8GxE3zVj3XESM5be9jahHkqQpdXdGiYjzgI8CW4BjwH0RsTeldGjaZv8ADAC/P8uX+ElKqafeOiRJmk0jWoBdAxxOKR0BiIjPATcCPwu6lNJ4vu50A95PkqR5a8Suy8uAo9OeH8uXzdcLI2IkIr4ZEW+ca6OIuDXfbuT48eOLLFWS1Gna4WSUX0wp9QL/DvhIRPzSbBullPaklHpTSr0rV87aoFqSpJ/TiKB7FFgz7fnl+bJ5SSk9mt8fAYaBVzagJkklMDFRdAXqBI0IuvuAqyLiyohYBtwMzOvsyYi4KCLOzx9fAvw6047tSaquQ4dg7VoYGiq6ElVd3SejpJSejYh3AXcB5wF/klJ6ICLeD4yklPZGxKuALwEXAa+PiN0ppZcB64Hb85NUlgAfnHG2pqQKGR/PbqdPQ38/LF8O27cXXZWqriETr6aU/gr4qxnL/vu0x/eR7dKc+br/C/xKI2qQ1P4GB2H37uefb9oEHnJXs7XDySiSOsTAABw8CPv3w5o1MDoKk5NFV6WqM+gktUytBn192Uhu3z44eTK7l5qpIbsuJWmhNmyAI0egu7voSlR1jugkFcaQUysYdJKkSjPoJEmVZtBJkirNoJMkVZpBJ0mqNINOklRpBp0kqdIMOklSpRl0kqRKM+gkSZVm0EmSKs2gkyRVmkEnSao0g06SVGkGnSSp0gw6SVKlGXSSpEoz6CRJlWbQSSU3NDTE5OQkAJOTkwwNDRVckdRelhZdgKTFm5iYYMeOHXR1ddHT08PY2BgnT55ky5YtdHd3F12e1BYc0Ukl1t3dzejoKCtWrODgwYOsWLGC0dFRQ06axqCTSm79+vUMDg4CMDg4yPr164stSGozBp1UAUuWLDnjXtLz/KmQKqBWq7Fz505qtVrRpUhtJ1JKRdewYL29vWlkZKToMiRJbSIi7k8p9c62zhGdJKnSDDpJUqUZdJKkSjPoJEmVZtBJkirNoJMkVZpBJ0mqNINOklRpBp1UQU7dIz2vIUEXETdExPcj4nBE3DbL+tdGxLcj4tmIuGnGuv6IeDi/9TeiHqk49wJ/mN8XY2rqnnXr1rFp0ybWrVvHjh07mJiYKKwmqUh1B11EnAd8FNgGbAB+KyI2zNjsH4AB4M9nvPZiYCfwauAaYGdEXFRvTVIx7gU2A+/L74sJO6fukc7UiBHdNcDhlNKRlNIp4HPAjdM3SCmNp5S+A5ye8dqtwN0ppSdTSj8C7gZuaEBNUgGGgVPAc/n9cGGVOHWP9LxGBN1lwNFpz4/lyxr62oi4NSJGImLk+PHjiypUaq4+YBlwXn7fV2QxTt0j5UrzE5BS2pNS6k0p9a5cubLocqRZXAvsBz6Q319baDVO3SNlljbgazwKrJn2/PJ82Xxf2zfjtcMNqEkqyLUUHXBTarUau3btKroMqXCNGNHdB1wVEVdGxDLgZmDvPF97F3B9RFyUn4Ryfb5MkqSGqDvoUkrPAu8iC6gHgTtSSg9ExPsj4g0AEfGqiDgG/CZwe0Q8kL/2SbL9PPflt/fnyyRJaghnGJcklZ4zjEuSOpZBJ0mqNINOklRpBp0kqdIMOknSWRTfqLxejbhgXFJT3UvWR6GPdrkYXZ1iqlH5KbK2dsV3/FkMR3RSW2uPGRHUHlo/z+Aw7dKovB6O6KS2NszP/6Ip31/Uqt/UPINdXV309PQwNjbGyZMn2bJlSxOnYOojG8lNjej6mvQ+zeWITmprfbTTjAgqTjHzDLZXo/LFMuiktlaNXzRqjGLmGbwWeA9l/t5z16XU9tpnRgQVz3kGF85PSpJKxHkGF86mzpKk0rOpsySpYxl0kqRKM+gkSZVm0EmSKs2gkyRVmkEnSTqHcs9g4AXjkqSzKP8MBo7oJElnMUzZZzAw6CRJZ9FH2RuLu+tSknQWU43Fhynr5L8GnSTpHMrdWNxdl1IJTUwUXYFUHgadVDKHDsHatTA0VHQlUjm461IqgfHx7Hb6NPT3w/LlsH170VVJ5WDQSSUwOAi7dz//fNMmWLmysHKkUnHXpVQCAwNw8CDs3w9r1sDoKExOFl2VVA4GnVQCtRr09WUjuX374OTJ7F4qv+a3F3PXpVQyGzbAkSPQ3V10JVK9WtNezBGdVEKGnKphmFa0FzPoJEkF6aMV7cXcdSlJKkhr2osZdJKkAjW/vZi7LiVJlWbQSR3OvpmquoYEXUTcEBHfj4jDEXHbLOvPj4jP5+u/FRG1fHktIn4SEWP57WONqEcqXvOvDWoE+2aqE9R9jC4izgM+CmwBjgH3RcTelNKhaZu9A/hRSumlEXEz8CHgzfm6R1JKPfXWIbWP1lwbtFj2zVSnacSI7hrgcErpSErpFPA54MYZ29wITP3N+EVgc0REA95bakPDtOLaoMUaHISNG2HzZjh2DHp6iuub6W5TtUIjgu4y4Oi058fyZbNuk1J6Fvgn4MX5uisjYjQivh4Rr5nrTSLi1ogYiYiR48ePN6BsqVn6aMW1QYvVLn0z3W2qVin68oLHgCtSSk9ExK8BX46Il6WUfjxzw5TSHmAPQG9vb2pxndICtObaoMWq1bIbZP0yr746u7/llua/t7tNVYRGBN2jwJppzy/Pl822zbGIWAq8CHgipZSAZwBSSvdHxCPALwMjDahLKlDzrw1qhFb3zXS6IRWhEbsu7wOuiogrI2IZcDOwd8Y2e4H+/PFNwIGUUoqIlfnJLETEWuAq4EgDapI0T63sm9kuu03VWeoOuvyY27uAu4AHgTtSSg9ExPsj4g35Zp8EXhwRh4HfA6YuQXgt8J2IGCM7SeW3U0pP1luTVEodcGaG0w2pCJHtPSyX3t7eNDLi3k1VyNAQ7NiRDXHWry+6mpaZmHAmBjVGRNyfUuqdbZ2dUaR2sG0bdHXB1q1w4AAMD2dnbVScIadWMOikdrBqVXZB29Gj2QVuGzdmZ25IqlvRlxeoIsZPjDM4NshAzwC1C2tFl1M+k5MwNgZXXJEFXMTz1wBIqotBp4YYPzHO7q/vpq/WZ9Atxp13Zmdm3HNPRx2jk1rBXZdqiNPp9Bn3WqD+/uyCNkNOajiDTnU7dPwQA18eAGDgywMcOn7o7C/Q7DwzQ2oKg051mXhqgqtvv5qnTz3Npis38fSpp7n69quZeKr614RJKgeP0aku3Rd0c/vrbmfbVdtYtXwVk/88yb7D++i+wNGJpPbgiE516+/pZ9XyVQCsWr6KW17Rgu7AdRoaGmIy7z01OTnJkC30pcpyRKeOMzExwY4dO+jq6qKnp4exsTFOnjzJli1b6PY4mVQ5jujUcbq7uxkdHWXFihUcPHiQFStWMDo6ashJFWXQqSOtX7+ewbzzyODgIOs9rV+qLINOHWvJkiVn3EuqJn/C1RTjJ8bZNbyL8RPjRZcyp1qtxs6dO6nZakuqNINOTTHVEqzdg27Xrl0GnVRxBp2awpZgktqFQaeGsyWYpHZi0KmhbAkmqd14wbgaypZgktpNpJSKrmHBent708jISNFlSJLaRETcn1LqnW2duy61YGW4dECSphh0WrAyXDqgzjPhYWDNwaDTgnnpgNrNoUOwdi04CYVmY9BpQbx0QO1ifByGh+HAAdi6FZYvh+3bi65K7cig07x56YDayeAgbNwImzfDsWPQ0wMrVxZdldqRlxdo3rx0QItzLzAM9AHXNuyrDgxAXx+cPp09Hh2FyUlYtaphb6GK8PICSU10L7AZOAUsA/bTyLCbcugQXH017NkDt7T/BPdqgrNdXuCITlITDZOF3HP5/TDNCLoNG+DIEXDuXM3GY3RqCa+961R9ZCO58/L7vqa9kyGnuRh0agmvvetU15LtrvwAzdptKZ2Luy7VEl5718muxYBTkRzRqena5to7W2dIHcmgU1O1zbV3Q0NZ64wHH2zt+0oVUea/E911qaZqm2vvtm2Drq6shcbgICxZArVadpN0Fvfy+OPD/NZv9fH2t19Lf3/R9Syc19Gpc2zaBAcPPv98507YtauwcqR2Nj4OTz55L694xWZSOsWpU8t45pn9XHRRex5v9To6Vd74iXEGxwYZ6BmgdmHt55a/fumN/NrYGFxxRTaii3A0J53F4CA888wwv/qrp1i6NLsOsqtrmDKeWGTQqRKmLl/oq/WdEXRP/uQveeaf/jd3/dFP6XnqJOfdcw+sX19coR1mYsLr28pqYACefLKPiGU8+2w2ovvxj/u4+OKiK1u4hpyMEhE3RMT3I+JwRNw2y/rzI+Lz+fpvRURt2rr35Mu/HxFbG1GPinYv8If5fWvMvHxhfBy+/e17+ZWV/4UP/MaT/Of/82GefugOQ66FnDqn3Go1uPrqaznvvP088cQH2LZtP1/9avlGc9CAEV1EnAd8FNgCHAPui4i9KaXp55C/A/hRSumlEXEz8CHgzRGxAbgZeBnQDXwtIn45pfRcvXWpKK3pbTjdzMsX9r11H3cMbsh3uzzL0qXPkXiGp1Yf5EW8oam1dLrx8ex2+jT09zt1TjVcy+rV1/LZz5Z3dN6IEd01wOGU0pGU0ingc8CNM7a5EZj6u+6LwOaIiHz551JKz6SUfgAczr+eSmuYn+9t2BiztRGb6/KF3u3f468v+BKnTid++tPz+Omp8/nNT/+ZUwo1mVPnVFdZQw4ac4zuMuDotOfHgFfPtU1K6dmI+Cfgxfnyb8547WWzvUlE3ArcCnDFFVc0oGw1Rx/ZSG5qRNfXsK8823G4uS5feN0rXs4Ty36H//fcJTz14+/wpjf9G9ZtOe2UQk3m1DlqR6U5GSWltAfYA9nlBQWXozlN9TYcptHzj83VRqy/5/kLe1YtX8Utr7jlzOX/4jdKvdulTKZfmrhvXzZ1zr59Tp2jYjVi1+WjwJppzy/Pl826TUQsBV4EPDHP16p0rgXeQyNDrt42YoZc601NnWPIqWiNCLr7gKsi4sqIWEZ2csneGdvsBab+7L4JOJCyK9X3AjfnZ2VeCVwF/E0DalKFtE0bMS2Yf2CoHdS96zI/5vYu4C6ySaf+JKX0QES8HxhJKe0FPgn8aUQcBp4kC0Py7e4ADgHPAr/jGZeaqW3aiEkqJVuASZJK72wtwJy9QJJUaQadJKnSDDpVWOtbkZ2h3Sbward6pBYx6FRRU63I3pfftzjs2m2i13arR2ohg04VNcysrchaNaqZPtHrgQMwPJw1gSxKu9UjtZBBp4rqI2tBdh4/a0XWylHNqlVZo8ejR7PGjxs3Zo0gi9Ju9UgtVJoWYOocc02iujCztCLb9kvPj2oGB2HJkjN7VjXS5CS000Sv7VaP1EKO6NRQQ0NDTE5OAjA5OcnQIiYjm2rePH2WgsWZ0YqslaOaO++EkyezRo8bN2adjhcYLA3dy9qAeqSyckSnhpmYmGDHjh10dXXR09PD2NgYJ0+eZMuWLXR3d897uum5mjfXrZWjmv5+2LJl0T2wDh3KGiLffnv2pYquRyozR3RqmO7ubkZHR1mxYgUHDx5kxYoVjI6OZiE3z+Nj9TZvPqtWj2oWGCrj49k5IgcOZHtXGz5pqSGnDmXQqaHWr1/PYL47cHBwkPXr12cr5nHWX9ObN/f3Z+30p2pqM05aKjWHuy7VcEuWLDnjHnj++NjBg9lvcoCdO2HXrp9t0pLmzW08qnHSUqk5bOqshhsfH2dwcJCBgQFqU7sGJydh3Tq44IIzj495QsSspo7R7dnjfG7SfJytqbMjOjVcrVZj17SRGvD88bF77mnbXYftZGrS0jYegEql4TE6tUabHx9rR4ac1BgGnVqn439zF9xkWupQ7rqUWmKqyfQpspZk+/nZheySmsoRndQSw8zaZFpS0xl0Ukv08XNNpiW1hLsupZaYpcm0pJYw6KSWuRYDTmo9d11KkirNoJMkVZpBJ0mqNINOkmZoxATCah+ejCJJ05xzAmGVjiM6SZrmrBMIq5QMOkkl1/geonNOIKxSctelpBJrXg/RWScQVin5PyipxIZpVg/RWq3Gzp07n588WKXliE5SifWRjeSmRnR9DfvKs04gzL3Yxq18DDpJJdbKHqJOtVRWBp2kkmtVD9Fhfn43qUFXBh6jk1ptYqLoCrQofTjVUjkZdFIrDQ3B2rXw4INFV6IFm9pN+gHcbVku7rqUWmnbNujqgq1bYXAQliyBWi27qQScaqmMHNFJrbRqFfT0wNGjsHkzbNyYBZ6kpqlrRBcRFwOfB2rAOPCmlNKPZtmuH/hv+dM/SCkN5cuHgUuBn+Trrk8pTdZTk9TWJidhbAyuuCILuAhHc1KT1bvr8jZgf0rpgxFxW/783dM3yMNwJ9ALJOD+iNg7LRDfklIaqbMOqRzuvBNOnoR77gHbSkktUe+uyxuBqfkrhoA3zrLNVuDulNKTebjdDdxQ5/uqg4yfGGfX8C7GT4wXXUr9+vvhyBFDTmqheoNudUrpsfzxD4HVs2xzGXB02vNj+bIpn4qIsYh4X0TEXG8UEbdGxEhEjBw/frzOslUm4yfG2f313dUIOgC74Estdc5dlxHxNeAls6x67/QnKaUUEWmB7/+WlNKjEXEB8BfA24BPz7ZhSmkPsAegt7d3oe+jEjudTp9xL0kLcc4RXUrpupTSy2e5fQV4PCIuBcjvZzuR5FFgzbTnl+fLSClN3T8F/DlwTX3/HFXNoeOHGPjyAAADXx7g0PFDxRYkqXTq3XW5F+jPH/cDX5llm7uA6yPiooi4CLgeuCsilkbEJQAR8QLgdcD36qxHCzQ0NMTkZPb3yeTkJENDQ+d4RetMPDXB1bdfzdOnnmbTlZt4+tTTXH371Uw8ZWcRSfNX71mXHwTuiIh3AH8PvAkgInqB304pvTOl9GREfAC4L3/N+/Nly8kC7wVkPXW+Bny8znq0ABMTE+zYsYOuri56enoYGxvj5MmTbNmypS1mU+6+oJvbX3c7267axqrlq5j850n2Hd5H9wXF1yapPCKl8h3u6u3tTSMjXpHQCA8++CBbt27l6NGjrFmzhrvuusvZlCWVTkTcn1LqnW2dnVE63Pr16xnMO3MMDg4acpIqx6ATS5YsOeNekqrE32yiVquxc+dOaraiklRBHqNrNxMTXlAsSQvkMbqyWOBcZZVqjSVJTWLQtZPpc5UdOADDwzA+PufmlWuNJUlNYNC1kwXOVWZrLEk6N4OunUyfq+zAATh4EAYGZt30XK2x2rnjiSS1Ur2dUbQI4yfGGRwbZKBngNqFtedXzHOusqnWWF0v6GLTlZsYfWyUq2+/miP/6QjdF3S3fccTSWolR3QFmPPY2jznKptqjfXQux5i/y37eehdD7Hn9Xt+1hqru7ub0dFRVqxYwcGDB1mxYgWjo6OGnKSOZNAV4KzH1uYZRv09/axavgqAVctXccsrbjljvR1PJClj0LVYK6edseOJJBl0LdXqaWfseCJJnozSUq2edqZWq7Fr166mfG1JKgtHdC12rmNrmsNEG0+22s61aVG8PKdaDDq1vwW2Rmupdq5NizJ1ec66devYtGkT69atY8eOHUz4B01pGXRqkXuBP8zvF2iBrdFaqp1r06J4eU71GHRqgXuBzcD78vsFht0CW6O1VDvXpkXz8pxq6diTUebsTqImGAZOAc/l98PAtfN/+fTWaIODEAHtciZpO9emunh5TnV0dNDt/vpu+mp95Qu60s1Z1wcsIwu5ZfnzBZhna7RCtHNtqouX51RHx/6pUtrO/6U8+eFaYD/wgfx+AaM5mHdrtEK0c22qy9TlOQZd+XXkDOOHjh/ihs/cwNEfH2XNL6xh31v3sWHlhgZW2ESTk7BuHaxYke0qW7Ik21XmD6OkDuYM49O0ujtJw3nygyQtSMcdo2t1d5KG8+QHSVqQjgs6yLqTTClddxJPfpCkBem4XZel58kPkrQgBl0ZlerSAkkqlkGnn7GVn6QqMugEwKFD2eV5NmmXVDUdeTKKMuPj2e306ezQ3/LlsH170VVJUmMZdB1scBB2737++aZNsHJlYeVIUlO467KDDQzAwYOwfz+sWQOjo9lleh2nZAcnS1auVDiDroPVatDXl43k9u3LLs/bt29+rx0/Mc6u4V2MnxhvYoUtULLeoR5LVVW0chb3jux1qdktZFKE4fFhNg5t5GD/QfpqfU2tq6lK0Dt05rHUkyfhoYfczazympiYYO3atXR1ddHT08PY2BgnT57kyJEji57g1l6XmpeFfH+VdvaHmUrQO3RwMCtr82Y4diwr15BTmbV6FneDTgt26PghBr48AMDAlwc4dPxQsQXVY3rv0AMHsoOWAwNFV3UGj6Wqilo5i7tBpwUp/ewPM031Dt23Lxs29fW11W5LqO9YqtTOWjWLu5cXaEFKP/vDTP39sGVLadqqbdiQtTotSbnSWbVqFve6TkaJiIuBzwM1YBx4U0rpR7Nstw/4l8Bfp5ReN235lcDngBcD9wNvSymdOtf7ejKKJGm6Zp6MchuwP6V0FbA/fz6bPwLeNsvyDwEfTim9FPgR8I4665Ek6Qz1Bt2NwNTFD0PAG2fbKKW0H3hq+rKICGAT8MVzvV6SpMWqN+hWp5Qeyx//EFi9gNe+GDiRUno2f34MuGyujSPi1ogYiYiR48ePL65aSVLHOefJKBHxNeAls6x67/QnKaUUEU27+jyltAfYA9kxuma9jySpWs4ZdCml6+ZaFxGPR8SlKaXHIuJSYCFX9zwBXBgRS/NR3eXAowt4vSRJ51Tvrsu9QH/+uB/4ynxfmLLTPQ8CNy3m9ZIkzUe9QfdBYEtEPAxclz8nInoj4hNTG0XEN4AvAJsj4lhEbM1XvRv4vYg4THbM7pN11iNJ0hnqumA8pfQEsHmW5SPAO6c9f80crz8CXFNPDZIknY0twAo2Nd3Ntx48WnQpklRJBl3Bxk+Ms/sLd/CansucY0ySmsBelwWZmmPs/kdfBJ+5ixd2Pcf27f7d0SwLmWtPUrX4m7UgU3OM/f5bXwk/XsOplX/DcUo83U0bc1ZuqbMZdAXZdtPjvODt17PinTdy/sWPc+rYBl75x1vLO91Nmxkfh+HhbIq5rVth+XLYvr3oqiQVwaAryKtfvpqP/+5beOQjH+fb31jNstMX0r/is+Wd7qbNOCu3pCl1TdNTlCpO0+MxpMaaOgZ6+nQ2Q/fTT8NDD8GqVQUXJqkpmjlNjxrEkGus0s3KPeEua6lZDDpV3tSs3LfcUnQlcxgays6WefDBoiuRKsmgU0do6xHztm3Q1ZWdNXPgQHYWzfh40VVJlWHQSUVbtSo7W+bo0ezsmY0bs7NpJDWEF4xLRZuchLExuOKKLOAisoOMkhrCoJOKdued2dky99wD69cXXY1UOe66lIrW35+dLWPISU1h0EntoK3PlpHKzaCTJFWaQSdJqjSDTpJUaQadJKnSDDpJUqUZdJKkSjPoJEmVZtBJkirNoJMkVZpBJ0mqNINOklRpBl0Hm5gougJJaj6DrkMdOgRr18LQUNGVSFJzOR9dBxkfz26nT2czwyxfDtu3F12VJDWXQddBBgdh9+7nn2/aBCtXFlaOJLWEuy47yMAAHDwI+/fDmjUwOgqTk0VXJUnNZdB1kFoN+vqykdy+fXDyZHYvSVXmrssOtWEDHDnixNaSqs8RXQcz5CR1AoNOklRpBp0kqdIMOklSpdUVdBFxcUTcHREP5/cXzbHdvog4ERFfnbF8MCJ+EBFj+a2nnnokSZqp3hHdbcD+lNJVwP78+Wz+CHjbHOv+a0qpJ7+N1VmPJElnqDfobgSmuiUOAW+cbaOU0n7gqTrfS5KkBas36FanlB7LH/8QWL2Ir/E/IuI7EfHhiDh/ro0i4taIGImIkePHjy+qWElS5zln0EXE1yLie7Pcbpy+XUopAWmB7/8eYB3wKuBi4N1zbZhS2pNS6k0p9a6saoPGss2bU7Z6JXWkcwZdSum6lNLLZ7l9BXg8Ii4FyO8X1DkxpfRYyjwDfAq4ZjH/iEoYGsrmzXnwwaIrmZ+y1SupY9W763Iv0J8/7ge+spAXTwvJIDu+97066ymvbdugqwu2boUDB2B4OJtTp12VrV5JHaveoPsgsCUiHgauy58TEb0R8YmpjSLiG8AXgM0RcSwituar/iwivgt8F7gE+IM66ymvVaugpweOHoXNm2HjxmxenXZVtnolday6mjqnlJ4ANs+yfAR457Tnr5nj9Zvqef9KmZyEsTG44oosMCKy6QbaVdnqldSxnL2gXdx5ZzZvzj33wPr1RVdzbmWrV1LHiuxkyXLp7e1NIyMjRZfReBMT5ZpSoGz1SqqsiLg/pdQ72zp7XbaTsoVG2eqV1JEMOklSpRl0kqRKM+gkSZVm0EmSKs2gkyRVmkEnSao0g06SVGkGnSSp0gw6SVKlGXSSpEoz6CRJlWbQSZIqzaCTJFWaQSdJqjSDTpJUaQadJKnSDDpJUqUZdJKkSjPoJEmVZtBJkirNoJMkVZpBJ0mqNINOklRpBp0kqdIMOklSpRl0kqRKM+gkSZVm0EmSKs2gkyRVmkEnSao0g06SVGkGnSSp0gw6SVKlGXSSpEqrK+gi4uKIuDsiHs7vL5plm56IuDciHoiI70TEm6etuzIivhURhyPi8xGxrJ56JEmaqd4R3W3A/pTSVcD+/PlMJ4FbUkovA24APhIRF+brPgR8OKX0UuBHwDvqrEeSpDPUG3Q3AkP54yHgjTM3SCn9XUrp4fzxBDAJrIyIADYBXzzb6yVJqke9Qbc6pfRY/viHwOqzbRwR1wDLgEeAFwMnUkrP5quPAZed5bW3RsRIRIwcP368zrIlSZ1i6bk2iIivAS+ZZdV7pz9JKaWISGf5OpcCfwr0p5ROZwO6+Usp7QH2APT29s75PpIkTXfOoEspXTfXuoh4PCIuTSk9lgfZ5Bzb/QLwl8B7U0rfzBc/AVwYEUvzUd3lwKML/hdIknQW9e663Av054/7ga/M3CA/k/JLwKdTSlPH40gpJeAgcNPZXi9JUj3qDboPAlsi4mHguvw5EdEbEZ/It3kT8FpgICLG8ltPvu7dwO9FxGGyY3afrLMeSZLOENnAqlx6e3vTyMhI0WVIktpERNyfUuqdbZ2dUSRJlWbQSZIqzaCTJFWaQSdJqjSDTpJUaQadJKnSDDpJUqUZdJKkSjPoJEmVZtBJkirNoJMkVZpBJ0mqNINOklRpBp0kqdIMOklSpRl0kqRKM+gkSZVm0EmSKq1jg278xDi7hnfxrQePFl2KJKmJOjrodn/hDl7TcxlDQ0VXI0lqlqVFF9Bq4+PZ7f5HXwSfuYsXdj3H9u0dm/eSVHkdF3SDg7B7N8ArATh11T0c5yJWsqHIsiRJTdJxQ5ltNz3OC95+PSveeSPnX/w4p45t4JV/vJWJpyaKLk2S1AQdF3SvfvlqPv67b+GRj3ycb39jNctOX0j/is/SfUF30aVJkpqg43ZdAvT39AOwagMcORJ0d//rgiuSJDVLx43oZup2ICdJldbxQSdJqjaDTpJUaQadJKnSDDpJUqUZdJKkSjPoJEmVZtBJkirNoJMkVZpBJ0mqNINOklRpBp0kqdIMOklSpUVKqegaFiwijgN/X8eXuAT4xwaVUwTrL06ZawfrL1qZ62/32n8xpbRythWlDLp6RcRISqm36DoWy/qLU+bawfqLVub6y1y7uy4lSZVm0EmSKq1Tg25P0QXUyfqLU+bawfqLVub6S1t7Rx6jkyR1jk4d0UmSOoRBJ0mqtMoGXURcHBF3R8TD+f1Fs2zTExH3RsQDEfGdiHjztHVXRsS3IuJwRHw+Ipa1W/35dvsi4kREfHXG8sGI+EFEjOW3npYU/vz711t/YZ//Amrvz7d5OCL6py0fjojvT/vsV7Wo7hvy9z0cEbfNsv78/LM8nH+2tWnr3pMv/35EbG1FvbPUt6j6I6IWET+Z9nl/rA1rf21EfDsino2Im2asm/X7qJXqrP+5aZ/93tZVvQAppUregP8J3JY/vg340Czb/DJwVf64G3gMuDB/fgdwc/74Y8B/aLf683WbgdcDX52xfBC4qZ0//3PUX9jnP8/vnYuBI/n9Rfnji/J1w0Bviz/v84BHgLXAMuBvgQ0ztvmPwMfyxzcDn88fb8i3Px+4Mv8655Wo/hrwvVbWu4jaa8CvAp+e/nN5tu+jMtSfr3u6qM9+vrfKjuiAG4Gh/PEQ8MaZG6SU/i6l9HD+eAKYBFZGRACbgC+e7fVNds76AVJK+4GnWlTTQiy6/jb4/OdT+1bg7pTSkymlHwF3Aze0prxZXQMcTikdSSmdAj5H9u+Ybvq/64vA5vyzvhH4XErpmZTSD4DD+ddrpXrqL9o5a08pjaeUvgOcnvHadvg+qqf+Uqhy0K1OKT2WP/4hsPpsG0fENWR/zTwCvBg4kVJ6Nl99DLisWYXOYUH1z+F/5LtkPxwR5zewtvmop/6iP//51H4ZcHTa85k1firflfO+Fv0yPlc9Z2yTf7b/RPZZz+e1zVZP/QBXRsRoRHw9Il7T7GLnqiu3kM+vLJ/92bwwIkYi4psR8caGVtYgS4suoB4R8TXgJbOseu/0JymlFBFzXkcREZcCfwr0p5ROt+qPxEbVP4f3kP2SXkZ2/cu7gfcvps65NLn+pmpy7W9JKT0aERcAfwG8jWyXj5rjMeCKlNITEfFrwJcj4mUppR8XXViH+MX8+30tcCAivptSeqTooqYrddCllK6ba11EPB4Rl6aUHsuDbHKO7X4B+EvgvSmlb+aLnwAujIil+V+OlwOPNrj8htR/lq89NSJ5JiI+Bfx+HaXO9R7Nqr/pn38Dan8U6Jv2/HKyY3OklB7N75+KiD8n2zXU7KB7FFgzo56Zn9nUNsciYinwIrLPej6vbbZF15+yA0XPAKSU7o+IR8iOv480veoz65qykM9vzu+jFqrr/3/a9/uRiBgGXkm2Z6xtVHnX5V5g6gymfuArMzfIz+T7EvDplNLU8SDyH5yDwE1ne32TnbP+s8l/QU8d73oj8L1GFjcPi66/DT7/+dR+F3B9RFyUn5V5PXBXRCyNiEsAIuIFwOtozWd/H3BVZGerLiM7WWPmGXDT/103AQfyz3ovcHN+VuOVwFXA37Sg5ukWXX9ErIyI8wDyUcVVZCd1tMp8ap/LrN9HTapzLouuP6/7/PzxJcCvA4eaVuliFX02TLNuZPvu9wMPA18DLs6X9wKfyB+/FfgpMDbt1pOvW0v2w34Y+AJwfrvVnz//BnAc+AnZvvWt+fIDwHfJfsl+BlhRsvoL+/wXUPvb8/oOA/8+X7YcuB/4DvAA8L9o0RmMwHbg78j+mn5vvuz9wBvyxy/MP8vD+We7dtpr35u/7vvAtlZ+r9RbP/Bv8896DPg28Po2rP1V+ff3P5ONoh842/dRWeoH/lX+e+Zv8/t3FFH/uW62AJMkVVqVd11KkmTQSZKqzaCTJFWaQSdJqjSDTpJUaQadJKnSDDpJUqX9fwBe0UMj4NunAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(X_train_pca[:,0],X_train_pca[:,1],c=\"black\", marker=\"1\")\n",
    "plt.scatter(X_ptr_pca[:,0],X_ptr_pca[:,1],c=\"green\", marker=\"2\")\n",
    "plt.scatter(X_ei_pca[:,0],X_ei_pca[:,1],c=\"blue\", marker=\"3\")\n",
    "plt.scatter(X_high_pca[:,0],X_high_pca[:,1],c=\"red\", marker=\"4\")\n",
    "plt.scatter(X_SVR_PCA[:,0],X_SVR_PCA[:,1],c=\"yellow\", marker=\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_test",
   "language": "python",
   "name": "py37_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
